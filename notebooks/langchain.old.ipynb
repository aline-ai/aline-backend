{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.llms import OpenAI, OpenAIChat\n",
    "from langchain.prompts import load_prompt\n",
    "# from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from readability import Document\n",
    "import lxml\n",
    "\n",
    "# idk why can't import \n",
    "def simplify(html, document_title=\"\"):\n",
    "  document = Document(html)\n",
    "  title = document.title()\n",
    "  if title == \"[no-title]\":\n",
    "      title = document_title\n",
    "  tree = lxml.html.fromstring(document.summary())\n",
    "  this_level: list[lxml.html] = [tree]\n",
    "  while this_level:\n",
    "      next_level = []\n",
    "      for elem in this_level:\n",
    "          if elem.tag not in (\"figure\", \"a\"):\n",
    "              elem.attrib.clear()\n",
    "          next_level.extend(elem)\n",
    "      this_level = next_level\n",
    "  while len(tree) == 1 and tree[0].tag != \"p\":\n",
    "      tree = tree[0]\n",
    "  text = f\"<h1>{title}</h1></br>\" + \"\".join([lxml.html.tostring(child).decode('utf-8') for child in tree]).replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\"\n",
    "notes = \"<h1>Summary of article</h1><ul><li>\"\n",
    "context = simplify(requests.get(url).text)\n",
    "# context = \"<html><body><p>The quick brown fox jumped over the lazy dog.</p></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_in_md = md(context, heading_style=\"atx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with deliminators disappearing\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "text_splitter = MarkdownTextSplitter()\n",
    "documents = text_splitter.create_documents([context_in_md])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "docsearch = Chroma.from_documents(documents, embeddings)\n",
    "# index = VectorstoreIndexCreator().from_loaders([context_in_md])\n",
    "qa = VectorDBQA.from_chain_type(\n",
    "  # llm=OpenAIChat(max_tokens=1024, verbose=True),\n",
    "  llm=OpenAI(\n",
    "    max_tokens=-1, \n",
    "    verbose=True\n",
    "  ), \n",
    "  chain_type=\"map_reduce\", \n",
    "  vectorstore=docsearch, \n",
    "  return_source_documents=True\n",
    ")\n",
    "qa.verbose = True\n",
    "qa.combine_documents_chain.verbose = True\n",
    "qa.combine_documents_chain.llm_chain.verbose = True\n",
    "# qa.combine_documents_chain.llm_chain.llm.model_name = \"text-curie-001\"\n",
    "qa.combine_documents_chain.combine_document_chain.llm_chain.verbose = True\n",
    "# qa.combine_documents_chain.llm_chain.llm.model_name = \"gpt-3.5-turbo\"\n",
    "qa.combine_documents_chain.combine_document_chain.llm_chain.llm = OpenAIChat(max_tokens=1024, verbose=True)\n",
    "qa.combine_documents_chain.combine_document_chain.llm_chain.llm.model_kwargs = {\"stop\": [\"===END===\"]}\n",
    "# qa.combine_documents_chain.combine_document_chain.llm_chain.llm.model_name = \"gpt-3.5-turbo\"\n",
    "qa.combine_documents_chain.llm_chain.prompt = load_prompt(\"../src/prompts/autocomplete/map.yaml\")\n",
    "qa.combine_documents_chain.combine_document_chain.llm_chain.prompt = load_prompt(\"../src/prompts/autocomplete/reduce.yaml\")\n",
    "\n",
    "# qa.combine_documents_chain.llm_chain.llm = OpenAI(max_tokens=1024, verbose=True, model=\"text-curie-001\")\n",
    "# qa.combine_documents_chain.llm_chain.llm.model_name = \"text-curie-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can use a lower model tbh\n",
    "template = \\\n",
    "\"\"\"\n",
    "Use the following portion of a long article to see if any of the text is relevant complete the incomplete notes. \n",
    "Return any relevant text *verbatim*.\n",
    "\n",
    "<!-- START OF ARTICLE SECTION -->\n",
    "{context}\n",
    "<!-- END OF ARTICLE SECTION -->\n",
    "\n",
    "<!-- START OF INCOMPLETE NOTES -->\n",
    "{question}\n",
    "<!-- END OF INCOMPLETE NOTES -->\n",
    "Relevant text verbatim, if any:\n",
    "\"\"\"\n",
    "# Given the following extracted parts of a long document and a question, complete the notes. \n",
    "# * If the answer is not found in the context, \"I don't know\" will be written instead\n",
    "stuffing_template = \\\n",
    "\"\"\"\n",
    "The following are a set of summaries from a long article used to complete a small section of notes below:\n",
    "\n",
    "<!-- START OF ARTICLE SUMMARIES -->\n",
    "{summaries}\n",
    "<!-- END OF ARTICLE SUMMARIES -->\n",
    "\n",
    "The following are a small section of notes written according to the following:\n",
    "* The notes are based on the context, *not* on prior knowledge\n",
    "  * An answer will *not* be written if the answer is not found in the long article\n",
    "* The notes start at the start token (===START===) and end at the end token (===END===)\n",
    "* Github-style markdown syntax will be used to format the notes\n",
    "  * Lists, which start with astericks (*) will be used dominantly to organize the notes\n",
    "  * Indents will be used to nest lists\n",
    "  * Headers, which start with hashes (#) will be used SPARINGLY to organize the notes\n",
    "* The notes will be elaborate and detailed, but will not generate new section headers\n",
    "* Each line will be kept short, simple and concise, and will not exceed 80 characters\n",
    "* Multiple clauses or sentences will ALWAYS be broken into multiple lines \n",
    "\n",
    "===START===\n",
    "{question}\n",
    "\"\"\"\n",
    "qa.combine_documents_chain.llm_chain.prompt = PromptTemplate(\n",
    "  input_variables=[\"context\", \"question\"], \n",
    "  template=template\n",
    ")\n",
    "qa.combine_documents_chain.combine_document_chain.llm_chain.prompt = PromptTemplate(\n",
    "  input_variables=[\"summaries\", \"question\"], \n",
    "  template=stuffing_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new VectorDBQA chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following portion of a long article to see if any of the text is relevant complete the incomplete notes. \n",
      "Return any relevant text *verbatim*.\n",
      "\n",
      "<!-- START OF ARTICLE SECTION -->\n",
      "# What is a Transformer?. An Introduction to Transformers and… | by Maxime | Inside Machine learning | Medium\n",
      "\n",
      "# What is a Transformer?\n",
      "\n",
      "# An Introduction to Transformers and Sequence-to-Sequence Learning for Machine Learning\n",
      "\n",
      "New deep learning models are introduced at an increasing rate and sometimes it’s hard to keep track of all the novelties. That said, one particular neural network model has proven to be especially effective for common natural language processing tasks. The model is called a Transformer and it makes use of several methods and mechanisms that I’ll introduce here. The papers I refer to in the post offer a more detailed and quantitative description.\n",
      "\n",
      "# **Part 1: Sequence to Sequence Learning and Attention**\n",
      "\n",
      "[The paper](https://arxiv.org/abs/1706.03762) ‘Attention Is All You Need’ describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence. (Well, this might not surprise you considering the name.)\n",
      "\n",
      "Seq2Seq models are particularly good at translation, where the sequence of words from one language is transformed into a sequence of different words in another language. A popular choice for this type of model is Long-Short-Term-Memory (LSTM)-based models. With sequence-dependent data, the LSTM modules can give meaning to the sequence while remembering (or forgetting) the parts it finds important (or unimportant). Sentences, for example, are sequence-dependent since the order of the words is crucial for understanding the sentence. LSTM are a natural choice for this type of data.\n",
      "\n",
      "Seq2Seq models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space (n-dimensional vector). That abstract vector is fed into the Decoder which turns it into an output sequence. The output sequence can be in another language, symbols, a copy of the input, etc.\n",
      "\n",
      "Imagine the Encoder and Decoder as human translators who can speak only two languages. Their first language is their mother tongue, which differs between both of them (e.g. German and French) and their second language an imaginary one they have in common. To translate German into French, the Encoder converts the German sentence into the other language it knows, namely the imaginary language. Since the Decoder is able to read that imaginary language, it can now translates from that language into French. Together, the model (consisting of Encoder and Decoder) can translate German into French!\n",
      "\n",
      "Suppose that, initially, neither the Encoder or the Decoder is very fluent in the imaginary language. To learn it, we train them (the model) on a lot of examples.\n",
      "\n",
      "A very basic choice for the Encoder and the Decoder of the Seq2Seq model is a single LSTM for each of them.\n",
      "\n",
      "You’re wondering when the Transformer will finally come into play, aren’t you?\n",
      "\n",
      "We need one more technical detail to make Transformers easier to understand: *Attention*. The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important. It sounds abstract, but let me clarify with an easy example: When reading this text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context.\n",
      "\n",
      "An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context.\n",
      "<!-- END OF ARTICLE SECTION -->\n",
      "\n",
      "<!-- START OF INCOMPLETE NOTES -->\n",
      "Definition of a transformer:\n",
      "* \n",
      "<!-- END OF INCOMPLETE NOTES -->\n",
      "Relevant text verbatim, if any:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following portion of a long article to see if any of the text is relevant complete the incomplete notes. \n",
      "Return any relevant text *verbatim*.\n",
      "\n",
      "<!-- START OF ARTICLE SECTION -->\n",
      "In other words, for each input that the LSTM (Encoder) reads, the attention-mechanism takes into account several other inputs at the same time and decides which ones are important by attributing different weights to those inputs. The Decoder will then take as input the encoded sentence and the weights provided by the attention-mechanism. To learn more about attention, see [this article](https://skymind.ai/wiki/attention-mechanism-memory-network). And for a more scientific approach than the one provided, read about different attention-based approaches for Sequence-to-Sequence models in [this great paper](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf) called *‘Effective Approaches to Attention-based Neural Machine Translation’.*\n",
      "\n",
      "# **Part 2: The Transformer**\n",
      "\n",
      "The paper ‘Attention Is All You Need’ introduces a novel architecture called Transformer. As the title indicates, it uses the attention-mechanism we saw earlier. Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).\n",
      "\n",
      "Recurrent Networks were, until now, one of the best ways to capture the timely dependencies in sequences. However, the team presenting the paper proved that an architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks! One improvement on Natural Language Tasks is presented by a team introducing BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805).\n",
      "\n",
      "So, what exactly is a Transformer?\n",
      "\n",
      "An image is worth thousand words, so we will start with that!\n",
      "\n",
      "*Figure 1: From ‘Attention Is All You Need’ by Vaswani et al.*The Encoder is on the left and the Decoder is on the right. Both Encoder and Decoder are composed of modules that can be stacked on top of each other multiple times, which is described by *Nx* in the figure. We see that the modules consist mainly of Multi-Head Attention and Feed Forward layers. The inputs and outputs (target sentences) are first embedded into an n-dimensional space since we cannot use strings directly.\n",
      "\n",
      "One slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word.\n",
      "\n",
      "Let’s have a closer look at these Multi-Head Attention bricks in the model:\n",
      "\n",
      "*Figure 2. From ‘Attention Is All You Need’ by Vaswani et al.*Let’s start with the left description of the attention-mechanism. It’s not very complicated and can be described by the following equation:\n",
      "\n",
      "Q is a matrix that contains the query (vector representation of one word in the sequence), K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. For the encoder and decoder, multi-head attention modules, V consists of the same word sequence than Q. However, for the attention module that is taking into account the encoder and the decoder sequences, V is different from the sequence represented by Q.\n",
      "\n",
      "To simplify this a little bit, we could say that the values in V are multiplied and summed with some attention-weights *a,* where our weights are defined by:\n",
      "<!-- END OF ARTICLE SECTION -->\n",
      "\n",
      "<!-- START OF INCOMPLETE NOTES -->\n",
      "Definition of a transformer:\n",
      "* \n",
      "<!-- END OF INCOMPLETE NOTES -->\n",
      "Relevant text verbatim, if any:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following portion of a long article to see if any of the text is relevant complete the incomplete notes. \n",
      "Return any relevant text *verbatim*.\n",
      "\n",
      "<!-- START OF ARTICLE SECTION -->\n",
      "**Training**\n",
      "\n",
      "How to train such a ‘beast’? Training and inferring on Seq2Seq models is a bit different from the usual classification problem. The same is true for Transformers.\n",
      "\n",
      "We know that to train a model for translation tasks we need two sentences in different languages that are translations of each other. Once we have a lot of sentence pairs, we can start training our model. Let’s say we want to translate French to German. Our encoded input will be a French sentence and the input for the decoder will be a German sentence. However, the decoder input will be shifted to the right by one position. ..Wait, why?\n",
      "\n",
      "One reason is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character.\n",
      "\n",
      "If we don’t shift the decoder sequence, the model learns to simply ‘copy’ the decoder input, since the target word/character for position *i* would be the word/character *i* in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position *i* having only seen the word/characters *1, …, i-1* in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence. In a moment, we’ll see how that is useful for inferring the results.\n",
      "\n",
      "This is true for Seq2Seq models and for the Transformer. In addition to the right-shifting, the Transformer applies a mask to the input in the first multi-head attention module to avoid seeing potential ‘future’ sequence elements. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position.\n",
      "\n",
      "The process of feeding the correct shifted input into the decoder is also called Teacher-Forcing, as described in [this blog](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/).\n",
      "\n",
      "The target sequence we want for our loss calculations is simply the decoder input (German sentence) without shifting it and with an end-of-sequence token at the end.\n",
      "<!-- END OF ARTICLE SECTION -->\n",
      "\n",
      "<!-- START OF INCOMPLETE NOTES -->\n",
      "Definition of a transformer:\n",
      "* \n",
      "<!-- END OF INCOMPLETE NOTES -->\n",
      "Relevant text verbatim, if any:\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following portion of a long article to see if any of the text is relevant complete the incomplete notes. \n",
      "Return any relevant text *verbatim*.\n",
      "\n",
      "<!-- START OF ARTICLE SECTION -->\n",
      "**Changes to the model from the paper**\n",
      "\n",
      "As a first step, we need to remove the embeddings, since we already have numerical values in our input. An embedding usually maps a given integer into an n-dimensional space. Here instead of using the embedding, I simply used a linear transformation to transform the 11-dimensional data into an n-dimensional space. This is similar to the embedding with words.\n",
      "\n",
      "We also need to remove the SoftMax layer from the output of the Transformer because our output nodes are not probabilities but real values.\n",
      "\n",
      "After those minor changes, the training can begin!\n",
      "\n",
      "As mentioned, I used teacher forcing for the training. This means that the encoder gets a window of 24 data points as input and the decoder input is a window of 12 data points where the first one is a ‘start-of-sequence’ value and the following data points are simply the target sequence. Having introduced a ‘start-of-sequence’ value at the beginning, I shifted the decoder input by one position with regard to the target sequence.\n",
      "\n",
      "I used an 11-dimensional vector with only -1’s as the ‘start-of-sequence’ values. Of course, this can be changed and perhaps it would be beneficial to use other values depending of the use case but for this example, it works since we never have negative values in either dimension of the input/output sequences.\n",
      "\n",
      "The loss function for this example is simply the mean squared error.\n",
      "\n",
      "## **Results**\n",
      "\n",
      "The two plots below show the results. I took the mean value of the hourly values per day and compared it to the correct values. The first plot shows the 12-hour predictions given the 24 previous hours. For the second plot, we predicted one hour given the 24 previous hours. We see that the model is able to catch some of the fluctuations very well. The root mean squared error for the training set is 859 and for the validation set it is 4,106 for the 12-hour predictions and 2,583 for the 1-hour predictions. This corresponds to a mean absolute percentage error of the model prediction of 8.4% for the first plot and 5.1% for the second one.\n",
      "\n",
      "Figure 3: 12-hour prediction given previous 24 hours over one yearFigure 4: 1-hour prediction given previous 24 hours over one year# Summary\n",
      "\n",
      "The results show that it would be possible to use the Transformer architecture for time-series forecasting. However, during the evaluation, it shows that the more steps we want to forecast the higher the error will become. The first graph (Figure 3) above has been achieved by using the 24 hours to predict the next 12 hours. If we predict only one hour, the results are much better as we see on the second the graph (Figure 4).\n",
      "\n",
      "There’s plenty of room to play around with the parameters of the Transformer, such as the number of decoder and encoder layers, etc. This was not intended to be a perfect model and with better tuning and training, the results would probably improve.\n",
      "\n",
      "It can be a big help to accelerate the training using GPUs. I used the Watson Studio Local Platform to train my model with GPUs and I let it run there rather than on my local machine. You can also accelerate the training using Watson’s Machine Learning GPUs which are free up to a certain amount of training time! Check out [my previous blog](/inside-machine-learning/leveraging-watsons-machine-learning-gpus-to-accelerate-your-deep-learning-project-in-python-e4ccf497a5f3) to see how that can be integrated easily into your code.\n",
      "\n",
      "Thank you very much for reading this and I hope I was able to clarify a few notions to the people who are just starting to get into Deep Learning!\n",
      "<!-- END OF ARTICLE SECTION -->\n",
      "\n",
      "<!-- START OF INCOMPLETE NOTES -->\n",
      "Definition of a transformer:\n",
      "* \n",
      "<!-- END OF INCOMPLETE NOTES -->\n",
      "Relevant text verbatim, if any:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max_tokens set to -1 not supported for multiple inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# query=f'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# The following are notes according to the following:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# * The notes are based on the context, not on prior knowledge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Definition of a transformer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# * '''\u001b[39;00m\n\u001b[1;32m     14\u001b[0m notes \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mDefinition of a transformer:\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m* \u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mlstrip()\n\u001b[0;32m---> 17\u001b[0m s \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: notes})\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/base.py:168\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 168\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    160\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    161\u001b[0m     inputs,\n\u001b[1;32m    162\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    163\u001b[0m )\n\u001b[1;32m    164\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    167\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/vector_db_qa/base.py:160\u001b[0m, in \u001b[0;36mVectorDBQA._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msearch_type of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type\u001b[39m}\u001b[39;00m\u001b[39m not allowed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m answer, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mcombine_docs(docs, question\u001b[39m=\u001b[39;49mquestion)\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer, \u001b[39m\"\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/combine_documents/map_reduce.py:139\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m, docs: List[Document], token_max: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[1;32m    134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[39m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    140\u001b[0m         \u001b[39m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    141\u001b[0m         [{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdocument_variable_name: d\u001b[39m.\u001b[39;49mpage_content}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs} \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m docs]\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_results(results, docs, token_max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/llm.py:117\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m    116\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/chains/llm.py:59\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[0;32m---> 59\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/llms/base.py:128\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/llms/base.py:125\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    122\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m    126\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/llms/openai.py:251\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39m# TODO: write a unit test for this\u001b[39;00m\n\u001b[1;32m    250\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invocation_params\n\u001b[0;32m--> 251\u001b[0m sub_prompts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_sub_prompts(params, prompts, stop)\n\u001b[1;32m    252\u001b[0m choices \u001b[39m=\u001b[39m []\n\u001b[1;32m    253\u001b[0m token_usage: Dict[\u001b[39mstr\u001b[39m, \u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/aline-v5AmhfmB-py3.10/lib/python3.10/site-packages/langchain/llms/openai.py:336\u001b[0m, in \u001b[0;36mBaseOpenAI.get_sub_prompts\u001b[0;34m(self, params, prompts, stop)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    335\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(prompts) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 336\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_tokens set to -1 not supported for multiple inputs.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m         )\n\u001b[1;32m    339\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens_for_prompt(prompts[\u001b[39m0\u001b[39m])\n\u001b[1;32m    340\u001b[0m sub_prompts \u001b[39m=\u001b[39m [\n\u001b[1;32m    341\u001b[0m     prompts[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[1;32m    342\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(prompts), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m    343\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: max_tokens set to -1 not supported for multiple inputs."
     ]
    }
   ],
   "source": [
    "# query=f'''\n",
    "# The following are notes according to the following:\n",
    "# * The notes are based on the context, not on prior knowledge\n",
    "# * The notes start at the start token (===START===) and end at the end token (===END===)\n",
    "# * The notes use Github-style markdown syntax\n",
    "# * The notes are short and concise\n",
    "# * The notes use lists and headings to organize your response where possible\n",
    "# * The notes keep each line short, simple and concise \n",
    "# * The notes keep every line to one sentence and break into multiple lines whenever possible\n",
    "\n",
    "# ===START===\n",
    "# Definition of a transformer\n",
    "# * '''\n",
    "notes = f\"\"\"\n",
    "Definition of a transformer:\n",
    "* \"\"\".lstrip()\n",
    "s = qa({\"query\": notes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of a transformer:\n",
      "* There is no definition of a transformer explicitly given in the article.\n"
     ]
    }
   ],
   "source": [
    "print(notes + s[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of a transformer:\n",
    "* A model architecture consisting of an Encoder and a Decoder \n",
    "* Uses Multi-Head Attention and Feed Forward layers \n",
    "* Does not use Recurrent Networks like GRU and LSTM \n",
    "* Requires positional encoding of words in sequence \n",
    "* Works best for translation tasks and natural language tasks \n",
    "* Uses a mask to avoid seeing \"future\" sequence elements \n",
    "* Applies \"Teacher-Forcing\" during training \n",
    "* Loss function used is mean squared error \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Definition of a transformer:\n",
    "* A neural net architecture for transforming one sequence into another using an Encoder and a Decoder\n",
    "* Uses an attention mechanism to decide at each step which parts of the sequence are important\n",
    "* Does not use Recurrent Networks (RNNs) like LSTM or GRU\n",
    "\n",
    "How a Seq2Seq model works:\n",
    "* A neural net that transforms a given sequence into another sequence\n",
    "* Popular for translation tasks like converting a sequence of words in one language to another\n",
    "* Consists of an Encoder and a Decoder\n",
    "* Encoder maps input sequence to higher dimensional space, then feeds abstract vector to Decoder\n",
    "* Decoder turns vector into output sequence which could be another language or copy of input\n",
    "\n",
    "How an attention-mechanism works:\n",
    "* Looks at input sequence and decides at each step which parts are important\n",
    "* For example, when reading a text, focus on current word and hold important keywords in memory for context\n",
    "\n",
    "How the training for translation tasks works:\n",
    "* Need sentence pairs in different languages to train model\n",
    "* Encoder input is sentence in one language, decoder input is shifted sentence in other language\n",
    "* Allows model to predict next word/character given encoder sequence and previously seen decoder sequence\n",
    "* Fills first position of decoder input with start-of-sentence token and appends end-of-sentence token to decoder and target output sentence\n",
    "\n",
    "Example of how transformers can be used for time-series forecasting:\n",
    "* Used teacher forcing for training\n",
    "* Encoder gets window of 24 data points and decoder input is window of 12 data points with start-of-sequence value\n",
    "* Shift decoder input by one position with regard to target sequence to prevent copying task\n",
    "* Loss function is mean squared error\n",
    "* Accuracy decreases as more steps are forecasted\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('aline-v5AmhfmB-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "647ad36cd1ba1b19d126e26c7f4a0bab04f4682de41c82337fd1290abd115c76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
